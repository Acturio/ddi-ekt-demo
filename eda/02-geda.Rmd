# Análisis exploratorio

Una vez que los datos individuales han sido estudiados descriptivamente de manera cruda, se procede a realizar la creación de un único conjunto de datos que centralice la información de todas las fuentes de información presentadas anteriormente. Este conjunto se crea mediante agregaciones diarias de todas las métricas reportadas originalmente, de modo que se pueda analizar posteriormente la relación entre la variable de respuesta (transacciones diarias) y el resto de información que podría servir para estimar este pronóstico.

## Nomenclatura

Para el caso de Facebook y Google Ads, las variables que se incluyen en este conjunto de datos presenta una nomenclatura particular que depende de la información que se analiza en cada columna para cada campaña. La nomenclatura es la siguiente

$$\text{origen_métrica_propósito}$$

**Donde:**

> Origen: Es la fuente de datos (Google Ads, Google Analytics, Facebook).
>
> Métrica: Hace referencia a la medición (likes, costo, alcance, conversiones, etc).
>
> Propósito: Refiere al objetivo que tiene cada campaña.

En el caso de Google Analytics, la nomenclatura es ligeramente distinta, pues no contienen información de campañas sino de resultados de las sesiones en la plataforma de Elektra. Esta nomenclatura sigue la siguiente sintaxis:

$$\text{origen_métrica}$$

Ejemplos: 

* **fb_costo_video_views:** Variable que proviene de Facebook y mide el costo invertido cada día en campañas que tenían como propósito lograr vistas mediante videos

* **gtics_transacciones:** Variable proveniente de Google Analytics que mide la cantidad de transacciones diarias.

* **gtics_n_fuente_medio:** Variable de Google Analytics que refiere al número de combinaciones distintas de fuentes/medios por los cuales se llevaron a cabo las sesiones del portal de Elektra cada día

* **fb_n_link_click:** En Facebook, es el número de campañas distintas implementadas cada día cuyo propósito es la obtención de click en un link.

___

Una vez que el conjunto de datos ha sido creado, se procede a analizar la consistencia, calidad y disponibilidad de información obtenida. A partir de este análisis se conoce la viabilidad de usar la información de cada una de las fuentes de información. 

Algunas de las principales características de interés es la distribución univariada y multivariada de cada par de características, así como las medidas de tendencia central y porcentaje de datos perdidos.

## Análisis de disponibilidad

Aunque originalmente no se observaron muchos datos faltantes cuando se realiza el análisis individual de cada fuente de información, al hacer coincidir las tablas por fechas es posible tener disparidades importantes, por lo que se realiza un análisis de datos faltantes después de observar las fechas de disponibilidad de datos:

|Fuente           |Fecha inicial  |Fecha final |
|-----------------|---------------|------------|
|Facebook Ads     |2019-10-30     |2021-11-22  |
|Google Analytics |2019-09-30     |2022-06-13  |
|Google Ads       |2021-10-31     |2022-06-02  |

Al considerar el periodo completo del 30 de septiembre de 2019 al 13 de junio de 2022, se cuenta con un volumen de datos faltantes considerable, mismo que se representa en las siguientes gráficas:

```{r}
library(tidyverse)
library(corrplot)
library(heatmaply)
library(fpp3)
library(tsibble)
library(DataExplorer)
library(forecast)
library(skimr)
library(kableExtra)

fb_agg <- fb %>%
  group_by(dia = Dia, objetivo_fb = Objetivo) %>%
  summarise(
    fb_n = n_distinct(Nombre_campaña),
    fb_alcance = sum(Alcance, na.rm = T),
    fb_impresiones = sum(Impresiones, na.rm = T),
    fb_resultados = sum(Resultados, na.rm = T),
    fb_costo = sum(Importe_gastado_USD, na.rm = T),
    fb_clicks = sum(Clics_enlace, na.rm = T),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = objetivo_fb,
    values_from = c("fb_n", "fb_alcance", "fb_impresiones", 
                    "fb_resultados", "fb_costo", "fb_clicks"),
    values_fill = 0,
    names_vary = "slowest"
  ) %>% 
  rename_with(tolower)
  
gads_agg <- gads %>%
  mutate_at(vars(Clics:Conversiones),
            ~as.numeric(str_replace(.x, pattern = ",", replacement = ""))) %>%
  group_by(dia = Dia, objetivo_ga = Tipo_de_campaña) %>%
  summarise(
    gads_n = n_distinct(Campaña),
    gads_clicks = sum(Clics, na.rm = T),
    gads_impresiones = sum(Impr, na.rm = T),
    gads_costo = sum(Coste, na.rm = T),
    gads_conversiones = sum(Conversiones, na.rm = T),
    .groups = "drop"
  ) %>%
  pivot_wider(
    names_from = objetivo_ga,
    values_from = c("gads_n", "gads_clicks", "gads_impresiones", 
                    "gads_costo", "gads_conversiones"),
    values_fill = 0,
    names_vary = "slowest"
  ) %>% 
  rename_with(tolower)


gticks_tr_agg <- gticks_tr %>% 
  group_by(fecha = Fecha) %>%
  summarise(
    gtics_n_fuente_medio = n_distinct(Fuente_medio),
    gtics_sesiones = sum(Sesiones, na.rm = T),
    gtics_transacciones = sum(Transacciones, na.rm = T),
    gtics_usuarios = sum(Usuarios, na.rm = T),
    .groups = "drop"
  )
  
gticks_q_agg <- gticks_q %>% 
  group_by(fecha = Fecha) %>%
  summarise(
    gtics_n_productos = n_distinct(Producto),
    gtics_cantidad = sum(Cantidad, na.rm = T),
    gtics_ingresos = sum(Ingresos, na.rm = T),
    .groups = "drop"
  )
```
 
```{r}
data <- gticks_q_agg %>% 
  left_join(gticks_tr_agg, by = "fecha") %>% 
  left_join(gads_agg, by = c("fecha" = "dia")) %>% 
  left_join(fb_agg, by = c("fecha" = "dia")) %>% 
  rename_with(~str_replace_all(.x, " ", "_"))
```


```{r, eval=FALSE, out.height='700px'}
data %>% 
  select(starts_with("fb")) %>% 
  DataExplorer::plot_missing(
    geom_label_args = list(size = 2.7),
    theme_config= list(text = element_text(size=11)),
    title = "Missing Values % (Facebook)"
  ) +
  scale_y_continuous(limits = c(0, 1000))
```

```{r echo=FALSE, fig.align='center', out.width='700pt', out.height='700pt'}
knitr::include_graphics("img/01-eda/missings_google.png")
```

```{r, eval=FALSE, out.height='700px'}
data %>% 
  select(-starts_with("fb")) %>% 
  DataExplorer::plot_missing(
    geom_label_args = list(size = 2.7),
    theme_config= list(text = element_text(size=11)),
    title = "Missing Values % (Google)"
  )  +
  scale_y_continuous(limits = c(0, 1000))
```

```{r echo=FALSE, fig.align='center', out.width='700pt', out.height='700pt'}
knitr::include_graphics("img/01-eda/missings_facebook.png")
```
 
Es importante la cantidad de datos faltantes provenientes de Google Ads, por lo que se revisará posteriormente la factibilidad de conseguir dicha información. Una manera fácil de entender la magnitud de la falta de información se logra a través del siguiente gráfico:

```{r}
data %>% 
ggplot(aes(x = fecha, y = gtics_transacciones)) +
  geom_col(fill = "red", color = "red") +
  geom_vline(xintercept = ymd("2019-10-30"), colour = "blue") +
  geom_vline(xintercept = ymd("2022-06-16"), colour = "blue") +
  geom_vline(xintercept = ymd("2021-10-31"), colour = "green") +
  geom_vline(xintercept = ymd("2022-06-13"), colour = "green") +
  scale_x_date(date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  labs(title = "Monto de transacciones históricas", x = "Fecha", y = "Transacciones")
```

 Las transacciones diarias están representadas por las líneas verticales rojas (Google Analytics). El periodo contenido entre las lineas azules representa la información disponible de Facebook Ads, mientras que la información contenida entre las lineas verdes corresponde a Google Ads.
 

 
::: {.infobox .note data-latex="{note}"}
**¡Toma de decisión!**

Dada esta información disponible... Se comenzará a trabajar la información disponible en el periodo del 1° de mayo de 2020 al 1° de junio de 2022, sin embargo, es áltamente deseable contar con la información completa para extraer el mayor valor de la información recolactada.
:::

## Análisis univariado

Con el objetivo de conocer la distribución univariada de la variable de interés y de cada una de las características asociadas a las transacciones, se crean gráficos de histogramas, los cuales se presentan a continuación:

```{r} 
library(patchwork)

hist_trans <- data %>% 
  filter(fecha >= "2020-05-01") %>% 
  ggplot(aes(x = gtics_transacciones)) +
  geom_histogram(color = "blue", fill = "lightblue") +
  geom_vline(aes(xintercept = median(data$gtics_transacciones), color = "Mediana")) +
  geom_vline(aes(xintercept = mean(data$gtics_transacciones), color = "Media")) +
  scale_color_manual(name = "Estadísticas", values = c("Media" = "black", "Mediana" = "red")) +
  labs(title = "Histograma de transacciones", x = "Transacciones", y = "Conteos")

hist_log_trans <- data %>% 
  filter(fecha >= "2020-05-01") %>% 
  ggplot(aes(x = log(gtics_transacciones+1))) +
  geom_histogram(color = "blue", fill = "lightblue") +
  geom_vline(aes(xintercept = log(median(data$gtics_transacciones)+1), color = "Mediana")) +
  geom_vline(aes(xintercept = log(mean(data$gtics_transacciones)+1), color = "Media")) +
  scale_color_manual(name = "Estadísticas", values = c("Media" = "black", "Mediana" = "red")) +
  labs(title = "Histograma de transacciones", subtitle =  "(Escala logarítmica)", 
       x = "Transacciones", y = "Conteos")

hist_trans / hist_log_trans
```

A través de los histrogramas se observa que existe una cola pesada del lado derecho, el cual corresponde al alto número de transacciones que ocurre en la temporada del *buen fin* y *black friday*.

A través de los siguientes gráficos se analiza la distribución de las transacciones tanto en su escala original como en la escala logarítmica, mostrando la media, percentiles 10, 25, 50, 75, 90 y puntos que son considerados atípicos. 

```{r, out.height="400pt"} 
data_summary <- function(x) {
   m <- mean(x)
   ymin <- quantile(x, probs = 0.1) %>% unname()
   ymax <- quantile(x, probs = 0.9) %>% unname()
   return(c("y"=m,"ymin"=ymin,"ymax"=ymax))
}

violin_trans <- data %>% 
  filter(fecha >= "2020-05-01") %>% 
  ggplot(aes(x = 1, y = gtics_transacciones)) +
  geom_violin(fill = "purple") +
  geom_boxplot(width=0.1) +
  stat_summary(fun.data=data_summary, color = "red") +
  coord_flip() +
  scale_y_continuous(n.breaks = 10) +
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
  labs(title = "Distribución de transacciones", y = "Transacciones", x = "")

  
violin_log_trans <- data %>% 
  filter(fecha >= "2020-05-01") %>% 
  ggplot(aes(x = 1, y = log(gtics_transacciones+1))) +
  geom_violin(fill = "purple") +
    geom_boxplot(width=0.1) +
  stat_summary(fun.data=data_summary, color = "red") +
  coord_flip() +
  scale_y_continuous(n.breaks = 10) +
  theme(axis.text.y=element_blank(), axis.ticks.y=element_blank()) +
  labs(title = "Distribución de transacciones", subtitle =  "(Escala logarítmica)",
       y = "Transacciones", x = "")


violin_trans + violin_log_trans
```

Adicionalmente, se presenta un gráfico cuantil-cuantil (QQ-plot) que permite comparar la distribución normal con la distribución logarítmica del total de transacciones.

```{r}
qqplot_normal <- data %>% 
  filter(fecha >= "2020-05-01") %>% 
  ggplot(aes(sample = log(gtics_transacciones+1))) +
  geom_qq() +
  geom_qq_line(colour = "red") +
  ggtitle("QQ-Plot Normal") +
  xlab("Cuantiles de distribución normal") +
  ylab("Cuantiles de distribución de transacciones (log)")

df <- data.frame(y = log(data$gtics_transacciones+1))
params <- as.list(MASS::fitdistr(
  df$y, "chi-squared", start = list(df = 10), 
  method="Brent",lower=0.1,upper=100)$estimate
  )

df_param <- params[[1]] +1.5
qqplot_chi <- data %>% 
  filter(fecha >= "2020-05-01") %>% 
  ggplot(aes(sample = log(gtics_transacciones+1))) +
  geom_qq(distribution = qchisq, dparams = df_param) +
  geom_qq_line(distribution = qchisq, dparams = df_param, colour = "red") +
  ggtitle("QQ-Plot Chi Cuadrada") +
  xlab("Cuantiles de distribución Chi-cuadrada") +
  ylab("Cuantiles de distribución de transacciones (log)")

qqplot_normal + qqplot_chi

```

En este gráfico se puede apreciar que la distribución logarítmica de las transacciones se ajusta mejor a través de una familia paramétrica $\chi^2$ de 6 grados de libertad. Esto ayuda a entender los métodos aplicables a la distribución de transacciones, pues no se deberán usar aquellos que son usados con la distribución normal.

```{r, eval=FALSE}
data %>% 
  filter(fecha >= "2020-05-01") %>% 
  DataExplorer::plot_histogram(ncol = 2, title = "Univariate Histograms")
```

<!-- De manera adicional, los gráficos QQ-plot permiten conocer la bondad de ajuste de una distribución normal estándar con cada una de las potenciales variables explicativas al comparar la distribución de ambas variables (Variable explicativa Vs Distribución normal). -->

<!-- A continuación se muestran los gráficos que permiten entender mejor la comparación de distribuciones: -->

```{r, eval=FALSE}
data %>% 
  filter(fecha >= "2020-05-01") %>% 
  DataExplorer::plot_qq(
    ncol = 2, 
    nrow = 2, 
    title = "QQ-plots"
    )
```


```{r, out.width="800pt"}
library(shiny)

knitr::include_url(
  "https://acturio.shinyapps.io/ddi_eda_uni/",
  height = "600px")
```




## Análisis de correlación

Una vez analizada la distribución unitaria de cada una de las variables presentes, se desea conocer el grado de asociación entre variables, de forma que pueda saberse cuáles de ellas están relacionadas de manera positiva o negativa, así como la intensidad de esta posible relación.

```{r}
M = data %>%
  filter(fecha >= "2020-05-01") %>% 
  select(-fecha, -ends_with("lead_generation"), -ends_with("messages")) %>%
  cor(use = "pairwise.complete.obs")

```


```{r, out.height="800px"}
heatmaply_cor(
  M,
  xlab = "",
  ylab = "",
  column_text_angle = 90,
  k_col = 2,
  k_row = 2,
  scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
    low = scales::muted("red"),
    high = scales::muted("blue"),
    midpoint = 0,
    limits = c(-1, 1)
  )
)
```

Estas relaciones permiten tener una mayor idea de la consistencia y posibles variables que pueden usarse de manera conjunta para realizar un análisis predictivo. Es de particular interés conocer la relación existente entre la variable de respuesta **gtics_transacciones** y cada una de las variables presentes.

A partir de una reunión de entendimiento, se obtiene nueva información en cuanto al conjunto de variables cuya campaña es de *"shopping"*. Esta campaña cambió su nombre a *rendimiento máximo*, por lo que vale la pena analizar el comportamiento de esta campaña incluyendo la información de su versión anterior. A continuación se muestra nuevamente el correlograma que considera este cambio:

```{r}
data %<>% 
  rowwise() %>% 
  mutate(
    gads_n_rendimiento_máximo_v2 = sum(gads_n_rendimiento_máximo, gads_n_shopping, na.rm = T),
    gads_clicks_rendimiento_máximo_v2 = sum(gads_clicks_rendimiento_máximo, gads_clicks_shopping, na.rm = T),
    gads_impresiones_rendimiento_máximo_v2 = sum(gads_impresiones_rendimiento_máximo, gads_impresiones_shopping, na.rm = T),
    gads_costo_rendimiento_máximo_v2 = sum(gads_costo_rendimiento_máximo, gads_costo_shopping, na.rm = T),
    gads_conversiones_rendimiento_máximo_v2 = sum(gads_conversiones_rendimiento_máximo, gads_conversiones_shopping, na.rm = T),
  ) %>% 
  ungroup()
  

M2 = data %>%
  filter(fecha >= "2020-05-01") %>% 
  select(-ends_with("máximo"), -matches("shopping")) %>% 
  select(-fecha, -ends_with("lead_generation"), -ends_with("messages")) %>%
  map_df(replace_na, replace = 0) %>% 
  cor(use = "pairwise.complete.obs")
```

```{r, out.height="800px"}
heatmaply_cor(
  M2,
  xlab = "",
  ylab = "",
  column_text_angle = 90,
  k_col = 2,
  k_row = 2,
  scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
    low = scales::muted("red"),
    high = scales::muted("blue"),
    midpoint = 0,
    limits = c(-1, 1)
  )
)
```

```{r}
M2 %>% .[,"gtics_transacciones"] %>%
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename("Correlation" = ".", "Variable" = "rowname") %>% 
  arrange(desc(abs(Correlation))) %>% 
  mutate(Correlation = round(Correlation, 4)) %>% 
  DT::datatable()
```

En primer lugar, se proponen como variables explicativas a aquellas que tengan una correlación en valor absoluto mayor a 0.60, seguidas de aquellas cuya correlación sea mayor a 0.30. Finalmente, se explorará el impacto que pueda tener el resto de variables.

Es de particular interés estudiar las variables que puedan conocerse de manera anticipada al día de la predicción, es decir, variables como **costo invertido** son en primer instancia de mayor valor debido a que esas cantidades sí pueden planearse de manera anticipada, mientras que variables como **total de productos vendidos** sí presentan una alta correlación, sin embargo, no puede conocerse de manera precisa el valor de esta medición con anticipación.

## Análisis multivariado gráfico

Habiendo observado la correlación analítica de manera conjunta entre variable objetivo y el resto de variables, se procede a observar esta relación de manera gráfica.

```{r}
data %>%
  filter(fecha >= "2020-05-01") %>% 
  select(-fecha, -ends_with("lead_generation"), -ends_with("messages"), -ends_with("shopping"),-ends_with("máximo")) %>%
  mutate(Transacciones = gtics_transacciones) %>% 
  DataExplorer::plot_scatterplot(
    by = "Transacciones"
    )
```


<!-- ## Reducción de dimensión -->

<!-- Un caso de análisis particular que resulta de mucho interés, es el análisis de reducción de dimensión a partir de **Componentes Principales**, en donde es posible crear nuevas variables que sustituyan a las originales manteniendo un alto nivel de información con menos variables que el número original de variables. -->

<!-- En las siguientes gráficas se presenta el porcentaje de información retenida al conservar 1, 5, 10, etc componentes nuevas. Es posible apreciar en la gráfica que con 8 componentes se logra retener el 85% de información de las 32 variables originales. -->

```{r, eval=FALSE}
data %>%
  filter(fecha >= "2020-05-01") %>% 
  select(-fecha, -ends_with("lead_generation"), -ends_with("messages"), -ends_with("shopping"),-ends_with("máximo")) %>%
  na.omit() %>% 
  DataExplorer::plot_prcomp(
    variance_cap = 0.9,
    ncol = 2,
    nrow = 1
  )
```

## Análisis Temporal

Debido a que el objetivo del proyecto es realizar un pronóstico de las transacciones futuras con una semana y media de anticipación, resulta indispensable conocer principalemnte la relación entre las transacciones y el tiempo.

Este análisis se realiza estudiando la estacionalidad por días de la semana, semana del año, mes y años. 

```{r}
original_ts_plot <- data %>%
  filter(fecha >= "2020-05-01") %>% 
  rename(transacciones = gtics_transacciones) %>% 
  ggplot(aes(x = fecha, y = transacciones)) +
  geom_line() +
  scale_x_date(date_breaks = "1 month") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  scale_y_continuous(
    labels=function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    title = "Serie histórica de transacciones",
    x = "Tiempo",
    y = "Transacciones"
    )

plotly::ggplotly(original_ts_plot)
```

### Estacionalidad anual

Al analizar los patrones de comportamiento de transacciones anuales de manera cíclica es posible obtener un mayor entendimiento sobre meses o temporadas clave. Estos periodos pueden tener consistentemente transacciones bajas o altas y al conocer mejor este comportamiento podrán plantearse distintas estrategias comerciales.

Debido a la consistencia mostrada en los datos a través del tiempo, se toma la decisión de trabajar con información posterior al 1° de mayo de 2020 y hasta el 1° de junio de 2022.

```{r}
library(patchwork)

ts_full_data <- data %>% 
  filter(fecha >= "2020-05-01", fecha < '2022-06-01') %>%
  tsibble::as_tsibble(
  index = fecha,
  .drop = F
)

ts_full_data %>% saveRDS("data/ts_full_data.rds")

full_ts_plot <- ts_full_data %>%
  rename(transacciones = gtics_transacciones) %>% 
  gg_season(transacciones, labels = "left", period = "year") +
  scale_y_continuous(
    labels=function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    y = "Transacciones", x = "Trimestres",
    title = "Gráfico Estacional",
    subtitle = "Transacciones Elektra (Completo)"
    )

plotly::ggplotly(full_ts_plot) %>% 
  layout(title = list(
    text = paste0('Gráfico Estacional',
                  '<br>', 
                  '<sup>', 'Transacciones Elektra (Completo)','</sup>')
    )
  )
#full_ts_plot
```

```{r}
ts_full_data %>%
  as_tibble() %>% 
  rename(transacciones = gtics_transacciones) %>%
  select(fecha, transacciones) %>% 
  mutate(año = year(fecha)) %>% 
  group_by(año) %>% 
  summarise(
    `transacción media` = mean(transacciones, na.rm = T),
    `desviación media` = sd(transacciones, na.rm = T)/(sqrt(n())),
    .groups = "drop"
  ) %>% 
  mutate(
    `Lim inf` = `transacción media` - qnorm(1-0.10/2)*`desviación media`,
    `Lim sup` = `transacción media` + qnorm(1-0.10/2)*`desviación media`,
    lag1 = lag(`transacción media`),
    `cambio %` = (`transacción media`/lag1-1)*100
    ) %>% 
  select(-lag1) %>% 
  mutate_if(is.numeric, round, digits = 1) %>% 
  DT::datatable(options = list(dom = 't'))

```

Se puede apreciar en la tabla anterior que al considerar intervalos de confianza al 90% no existe una diferencia significativa en la media de transacciones anuales. El cambio porcentual entre un año y otro no es del todo comparable debido a que no son años completos. Mientras que en 2020 se analiza información posterior al 1° de mayo, en 2022 se cuenta con información hasta el 1° de mayo.

A fin de comparar el desempeño de las transacciones de ecommerce de una manera más justa, se realiza el análisis de la diferencia de transacciones realizadas tomando en cuenta el procentaje de avance del año. Esto, debido a que en 2022 no se ha llegado a la etapa del año donde se realizan más transacciones:

```{r}
ts_full_data %>%
  as_tibble() %>% 
  rename(transacciones = gtics_transacciones) %>%
  select(fecha, transacciones) %>% 
  mutate(año = year(fecha)) %>% 
  filter(
    año >= 2021,
    month(fecha) < 6
  ) %>% 
  group_by(año) %>% 
  summarise(
    `transacción media` = mean(transacciones, na.rm = T),
    `desviación media` = sd(transacciones, na.rm = T)/(sqrt(n())),
    .groups = "drop"
  ) %>% 
  mutate(
    `Lim inf` = `transacción media` - qnorm(1-0.10/2)*`desviación media`,
    `Lim sup` = `transacción media` + qnorm(1-0.10/2)*`desviación media`,
    lag1 = lag(`transacción media`),
    `cambio %` = (`transacción media`/lag1-1)*100
    ) %>% 
  select(-lag1) %>% 
  mutate_if(is.numeric, round, digits = 1) %>% 
  DT::datatable(options = list(dom = 't'))
```
De esta manera sí se puede apreciar que al comparar los años 2021 y 2022, hasta el 31 de mayo se cuenta con un aumento del 1% en transacciones medias. 

El siguiente análisis de estacionalidad anual se realiza considerando la intersección de información de transacciones y la información disponible de *Google Ads*.

```{r}
ts_plot <- ts_full_data %>%
  filter(fecha >= "2021-10-31", fecha < '2022-06-01') %>%
  gg_season(gtics_transacciones, labels = "both", period = "year") +
  scale_y_continuous(
    labels=function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    y = "Transacciones", x = "Trimsestres",
    title = "Gráfico Estacional"
    )


full_polar_plot <- ts_full_data %>%
  #filter(fecha >= '2021-10-31', fecha < '2022-06-01') %>%
  gg_season(gtics_transacciones, labels = "both", period = "year", polar = T) +
  scale_y_continuous(
    labels=function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    y = "Transacciones", x = "Trimsestres",
    title = "Gráfico Polar"
    )

ts_plot + full_polar_plot
```

El gráfico polar anterior permite detectar patrones de estacionalidad en fechas importantes. Entre los principales hallazgos, se encuentra:

* El periodo correspondiente a la segunda mitad del mes de Noviembre.

* Semana alrededor del 10 de Mayo

* Periodo (aproximado) correspondiente a la segunda mitad del mes de Julio.


### Estacionalidad semanal

Otro tipo de análisis cíclico de gran valor es el realizado cada semana. Para distinguir la tendencia central y dispersión a lo largo del tiempo se realizan las siguientes gráficas:


```{r}
ts_full_data %>%
  #filter(fecha >= "2021-10-31", fecha < '2022-06-01') %>%
  gg_subseries(gtics_transacciones, period = "week") +
  scale_y_continuous(
    labels=function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    y = "Transacciones", x = "Día de semana",
    title = "Gráfico Estacional: Transacciones Elektra"
  )
```

El gráfico anterior nos permite conocer la serie histórica de cada día de la semana. Adicionalmente se muestra la línea de transacciones promedio. Es importante mencionar que los días de gran volumen de ventas no debe asociarse al día de la semana per-se, sino al día festivo (10 de mayo, 25 de diciembre, etc) en que ocurre, el cual puede ser distinto cada año.

```{r}
mean_weekday <- ts_full_data %>% 
  as_tibble() %>% 
  mutate(weekday = lubridate::wday(fecha, label = T, abbr = F)) %>% 
  #summarise(media = mean(gtics_transacciones, na.rm = T))
  dplyr::group_by(`día semana` = weekday) %>% 
  dplyr::summarise(
    `transacción media` = mean(gtics_transacciones, na.rm = T),
    `desviación media` = sd(gtics_transacciones, na.rm = T)/(sqrt(n())),
    ) %>% 
  mutate(
    `Lim inf` = `transacción media` - qnorm(1-0.10/2)*`desviación media`,
    `Lim sup` = `transacción media` + qnorm(1-0.10/2)*`desviación media`,
  ) %>% 
  dplyr::arrange(desc(`transacción media`)) %>% 
  dplyr::mutate_if(is.numeric, round, digits = 2) 

mean_weekday %>% DT::datatable(options = list(dom = 't'))

```

En la tabla anterior se muestra analíticamente la media de las transacciones y la desviación media. Mediante estos valores, se construyen posteriormente intervalos de confianza que permitirán conocer la significancia en diferencia de transacciones por día de la semana.

```{r}
#119.73

ic_weekday_90 <- mean_weekday %>%
  #rename(weekday = `día semana`)
  ggplot(aes(x = `día semana`, y = `transacción media`)) +
  geom_point() +
  geom_errorbar(
    aes(ymin = `transacción media` - qnorm(1-0.1/2)*`desviación media`,
        ymax = `transacción media` + qnorm(1-0.1/2)*`desviación media`),
    width = 0.2) +
  geom_hline(yintercept = 119.73, colour = "red") +
  ggtitle("Transacciones por día de la semana, IC(90%)")

ic_weekday_85 <- mean_weekday %>%
  #rename(weekday = `día semana`)
  ggplot(aes(x = `día semana`, y = `transacción media`)) +
  geom_point() +
  geom_errorbar(
    aes(ymin = `transacción media` - qnorm(1-0.20/2)*`desviación media`,
        ymax = `transacción media` + qnorm(1-0.20/2)*`desviación media`),
    width = 0.2) +
  geom_hline(yintercept = 119.73, colour = "red") +
  ggtitle("Transacciones por día de la semana, IC(80%)")

ic_weekday_90 / ic_weekday_85
```

Es posible distinguir que existe una ligera diferencia en la media de transacciones al agregarlos por día de la semana, aunque con 90% de confianza no es lo suficiente como para afirmar que son significativamente distintas. Por otro lado, el intervalo de confianza al 80% del día miércoles se encuentra por debajo de la media global, por lo que en este caso sí contamos con una diferencia importante.

Es de interés adicional analizar la tendencia y varianza, pues podrían presentar sutiles diferencias también. Una prueba de hipótesis permitirá confirmar la proposición planteada.

```{r}
dfs <- ts_full_data %>% 
  as_tibble() %>% 
  mutate(weekday = lubridate::wday(fecha, label = T, abbr = F)) %>% 
  select(fecha, weekday, gtics_transacciones) %>% 
  dplyr::group_split(weekday) %>% 
  map(~mutate(.x, trend = row_number())) 

days_of_week <- dfs %>% map_df(~distinct(.x, weekday)) %>% pull()

dfs %>% 
  map(~lm(data = .x, gtics_transacciones~ trend)) %>% 
  map_df(tidy) %>% 
  mutate(weekday = rep(days_of_week, each = 2)) %>% 
  filter(term == "trend") %>% 
  arrange(desc(estimate)) %>% 
  select(-c(statistic, term)) %>% 
  relocate(weekday, .before = estimate) %>% 
  dplyr::mutate_if(is.numeric, round, digits = 4) %>% 
  DT::datatable(options = list(dom = 't'))
```

Debido a la gran variación en la tendencia, puede apreciarse que no existe un a diferencia significativa al analizarla por día de la semana.



### Autocorrelación

Las series de tiempo enfocadas en transacciones suelen en la gran mayoría de las ocasiones mostrar alta correlación con observaciones pasadas. Esto quiere decir que es común que las transacciones de un día se encuentren asociadas en volumen y tendencia con las transacciones de 1 día atrás o 7 días atrás. 

Las funciones de autocorrelación y autocorrelación parcial permiten observar el grado de correlación con *n* días hacia atrás. Este análisis permite explorar hasta qué punto las transacciones en un momento del tiempo pueden servir para predecir transacciones futuras a corto, mediano o largo plazo.

```{r}
acf_plot <- ts_full_data %>% 
  ACF(
    y = gtics_transacciones,
    lag_max = 15) %>% 
  autoplot() +
  ggtitle("Gráfico de Autocorrelación")

pacf_plot <- ts_full_data %>% 
  PACF(
    y = gtics_transacciones,
    lag_max = 15) %>% 
  autoplot() +
  ggtitle("Gráfico de Autocorrelación Parcial")

acf_plot + pacf_plot
```

Otra forma de analizar la autocorrelación es mediante el gráfico de dispersión de puntos de las transacciones diarios y compararlos contra las transacciones de días pasados (lags):

```{r}
ts_full_data %>%
  gg_lag(
    y = gtics_transacciones,
    geom="point",
    lags = 1:9
    ) +
  scale_y_continuous(
    labels=function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    y = "Transacciones", x = "Lag(Transacciones, n)",
    title = "Exploración autorregresiva") +
  theme(
    axis.text.x=element_blank(),
    axis.ticks.x=element_blank()
    )
```

### Descomposición aditiva

Para entender mejor el comportamiento de las transacciones, se procede a realizar una descomposición de la tendencia y la estacionalidad anual y semanal, de forma que se puedan observar los efectos que cada uno de estos factores aportan a la variación  de transacciones online.

```{r}
dcmp <- ts_full_data %>%
  model(stl = STL(gtics_transacciones))

trend_plot <- components(dcmp) %>%
  as_tsibble() %>%
  autoplot(gtics_transacciones, colour="gray") +
  geom_line(aes(y=trend), colour = "blue") +
  scale_y_continuous(
    labels=function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    y = "Transacciones",
    title = "Tendencia de total de transacciones online"
  )

season_adjust_plot <- components(dcmp) %>%
  as_tsibble() %>%
  autoplot(gtics_transacciones, colour="gray") +
  geom_line(aes(y=season_adjust), colour = "blue") +
  scale_y_continuous(
    labels=function(x) format(x, big.mark = ",", scientific = FALSE)) +
  labs(
    y = "Transacciones",
    title = "Estacionalidad ajustada al total de transacciones online"
  )

trend_plot / season_adjust_plot
```


```{r, out.height="600px"}
components(dcmp) %>% autoplot()
```












































